{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Files structure**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir - p datasets/{datapaths, EMO-DB, RAVDESS, TESS, CREMA-D, SAVEE, EMOVO, MELD/{train, test, dev}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Audio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data processing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion segregation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RAVDESS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ravdess_dir = \"datasets/RAVDESS/speech/\"\n",
    "dir_list = [ravdess_dir + folder + '/' for folder in os.listdir(ravdess_dir)]\n",
    "dir_list.sort()\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "for cur_dir in dir_list:\n",
    "    # retrieve list of recordings for consecutive actors\n",
    "    actor = os.listdir(cur_dir)\n",
    "    for cur_file in actor:\n",
    "        # retrieve names of consecutive files\n",
    "        part = cur_file.split('.')[0]\n",
    "        part = part.split('-')\n",
    "        # update the lists of filepaths and emotions' ids\n",
    "        file_emotion.append(int(part[2]))\n",
    "        file_path.append(cur_dir + cur_file)\n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotion'])\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "\n",
    "ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "ravdess_df.Emotion.replace(\n",
    "    {1: 'neutral', 2: 'calm', 3: 'happy', 4: 'sad',\n",
    "     5: 'angry', 6: 'fear', 7: 'disgusted', 8: 'surprised'},\n",
    "    inplace=True)\n",
    "ravdess_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CREMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crema_dir = 'datasets/CREMA-D/AudioWAV'\n",
    "crema_files = os.listdir(crema_dir)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "for cur_dir in crema_files:\n",
    "    file_path.append(crema_dir + '/' + cur_dir)\n",
    "    part = cur_dir.split('_')\n",
    "    file_emotion.append({\n",
    "        'SAD': 'sad',\n",
    "        'ANG': 'angry',\n",
    "        'DIS': 'disgusted',\n",
    "        'FEA': 'fear',\n",
    "        'HAP': 'happy',\n",
    "        'NEU': 'neutral'\n",
    "    }.get(part[2], 'Unknown'))\n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotion'])\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "crema_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TESS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_dir = 'datasets/TESS'\n",
    "tess_files = os.listdir(tess_dir)\n",
    "\n",
    "tess_emo_dict = {\n",
    "    'ps': 'surprised',\n",
    "    'disgust': 'disgusted'\n",
    "}\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "for cur_dir in tess_files:\n",
    "    if cur_dir.split('.')[-1] != 'wav':\n",
    "        continue\n",
    "    file_path.append(tess_dir + '/' + cur_dir)\n",
    "    part = cur_dir.split('.')[0]\n",
    "    part = part.split('_')[2]\n",
    "    file_emotion.append(tess_emo_dict.get(part, part))\n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotion'])\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "tess_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SAVEE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savee_dir = 'datasets/SAVEE/AudioData'\n",
    "savee_folders = os.listdir(savee_dir)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "for folder in savee_folders:\n",
    "    savee_files = os.listdir(savee_dir + '/' + folder)\n",
    "    for file_name in savee_files:\n",
    "        file_path.append(savee_dir + '/' + folder + '/' + file_name)\n",
    "        part = file_name[:2] if len(file_name) == 8 else file_name[0]\n",
    "        file_emotion.append({\n",
    "            'a': 'angry',\n",
    "            'd': 'disgusted',\n",
    "            'f': 'fear',\n",
    "            'h': 'happy',\n",
    "            'n': 'neutral',\n",
    "            'sa': 'sad',\n",
    "            'su': 'surprised'}.get(part, 'Unknown')\n",
    "        )\n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotion'])\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "savee_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EMO-DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_dir = 'datasets/EMO-DB/wav'\n",
    "emo_files = os.listdir(emo_dir)\n",
    "\n",
    "file_path = []\n",
    "file_emotion = []\n",
    "for file_name in emo_files:\n",
    "    file_path.append(emo_dir + '/' + file_name)\n",
    "    part = file_name.split('.')[0]\n",
    "    file_emotion.append({\n",
    "        'W': 'angry',\n",
    "        'L': 'neutral',\n",
    "        'E': 'disgusted',\n",
    "        'A': 'fear',\n",
    "        'F': 'happy',\n",
    "        'T': 'sad',\n",
    "        'N': 'neutral'\n",
    "    }.get(part[-2], 'Unknown'))\n",
    "\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotion'])\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "emo_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "emo_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join & save datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ravdess_df.to_csv('datasets/ravdess.csv', index=False)\n",
    "tess_df.to_csv('datasets/tess.csv', index=False)\n",
    "savee_df.to_csv('datasets/savee.csv', index=False)\n",
    "crema_df.to_csv('datasets/crema.csv', index=False)\n",
    "emo_df.to_csv('datasets/emo.csv', index=False)\n",
    "\n",
    "data_paths = pd.concat(\n",
    "    [ravdess_df, tess_df, savee_df, crema_df, emo_df], axis=0)\n",
    "data_paths.to_csv(\"datapaths/all_data_paths.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ravdess_df = pd.read_csv('drive/MyDrive/RAVDESS/ravdess.csv')\n",
    "tess_df = pd.read_csv('drive/MyDrive/TESS/tess.csv')\n",
    "savee_df = pd.read_csv('drive/MyDrive/SAVEE/savee.csv')\n",
    "crema_df = pd.read_csv('drive/MyDrive/CREMA-D/crema.csv')\n",
    "emo_df = pd.read_csv('drive/MyDrive/EMO/emo.csv')\n",
    "\n",
    "data_paths = pd.concat(\n",
    "    [ravdess_df, tess_df, savee_df, crema_df, emo_df], axis=0)\n",
    "data_paths.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_waveplot(data, sr, e):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.title(f'Waveplot for audio with {e} emotion', size=15)\n",
    "    librosa.display.waveplot(data, sr=sr)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_spectrogram(data, sr, e):\n",
    "    X = librosa.stft(data)\n",
    "    Xdb = librosa.amplitude_to_db(abs(X))\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.title(f'Spectrogram for audio with {e} emotion', size=15)\n",
    "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "def show_emotion(emotion):\n",
    "    path = np.array(data_paths.Path[data_paths.Emotion == emotion])[1]\n",
    "    data, sampling_rate = librosa.load(path)\n",
    "    create_waveplot(data, sampling_rate, emotion)\n",
    "    create_spectrogram(data, sampling_rate, emotion)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of emotions in whole dataset distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Count of emotions', size=16)\n",
    "sns.countplot(data_paths.Emotion)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particular emotions visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(show_emotion('fear'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(show_emotion('sad'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Happy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(show_emotion('happy'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Angry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(show_emotion('angry'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(show_emotion('calm'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Surprised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(show_emotion('surprised'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Disgusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(show_emotion('disgusted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neutral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(show_emotion('neutral'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(data):\n",
    "    noise_amp = 0.035 * np.random.uniform() * np.amax(data)\n",
    "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate)\n",
    "\n",
    "\n",
    "def shift(data):\n",
    "    shift_range = int(np.random.uniform(low=-5, high=5) * 1000)\n",
    "    return np.roll(data, shift_range)\n",
    "\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
    "\n",
    "\n",
    "path = np.array(data_paths.Path)[1]\n",
    "data, sample_rate = librosa.load(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear data (without augmentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "plt.title(\"Clear recording\", size=15)\n",
    "librosa.display.waveplot(y=data, sr=sample_rate)\n",
    "Audio(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmented data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = noise(data)\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.title(\"With random noise\", size=20)\n",
    "librosa.display.waveplot(y=x, sr=sample_rate)\n",
    "Audio(x, rate=sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = stretch(data)\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.title(\"Streched\", size=20)\n",
    "librosa.display.waveplot(y=x, sr=sample_rate)\n",
    "Audio(x, rate=sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = shift(data)\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.title(\"Shifted\", size=20)\n",
    "librosa.display.waveplot(y=x, sr=sample_rate)\n",
    "Audio(x, rate=sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pitch(data, sample_rate)\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.title(\"Pitch change\", size=20)\n",
    "librosa.display.waveplot(y=x, sr=sample_rate)\n",
    "Audio(x, rate=sample_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = np.array(data_paths.Path)[11]\n",
    "y, sr = librosa.load(path)\n",
    "some_voice, _ = librosa.effects.trim(y)\n",
    "librosa.display.waveplot(some_voice, sr=sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amplitude to dB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_length = 512\n",
    "D = np.abs(librosa.stft(some_voice, n_fft=2048, hop_length=hop_length))\n",
    "DB = librosa.amplitude_to_db(D, ref=np.max)\n",
    "plt.axis('off')\n",
    "librosa.display.specshow(\n",
    "    DB, sr=sr, hop_length=hop_length, x_axis='time', y_axis='log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power to dB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = librosa.feature.melspectrogram(\n",
    "    y=some_voice,\n",
    "    sr=sr,\n",
    "    hop_length=512,\n",
    "    win_length=512,\n",
    "    window=np.hanning(512))\n",
    "\n",
    "M_db = librosa.power_to_db(img_array, ref=np.max)\n",
    "plt.axis('off')\n",
    "img = librosa.display.specshow(M_db, y_axis='mel', x_axis='time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mel-spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(some_voice, sr=sr, n_fft=2048, hop_length=hop_length, n_mels=128)\n",
    "S_DB = librosa.power_to_db(S, ref=np.max)\n",
    "plt.axis('off')\n",
    "librosa.display.specshow(S_DB, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrogram dataset generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mel(sound, path_to_save, mode='stft'):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "\n",
    "    if mode == 'mel':\n",
    "        D = librosa.feature.melspectrogram(\n",
    "            some_sound, n_fft=N_FFT, hop_length=WIN_HOP_LENGTH, n_mels=256)\n",
    "        DB = librosa.power_to_db(D, ref=np.max)\n",
    "        librosa.display.specshow(\n",
    "            DB, sr=sampling_rate, hop_length=WIN_HOP_LENGTH, x_axis='time', y_axis='mel')\n",
    "    elif mode == 'stft':\n",
    "        D = np.abs(librosa.stft(some_sound, n_fft=N_FFT,\n",
    "                   hop_length=WIN_HOP_LENGTH))\n",
    "        DB = librosa.amplitude_to_db(D, ref=np.max)\n",
    "        librosa.display.specshow(\n",
    "            DB, sr=sampling_rate, hop_length=WIN_HOP_LENGTH, x_axis='time', y_axis='log')\n",
    "    else:\n",
    "        print(\"Wrong mode\")\n",
    "        return\n",
    "    fig.savefig(f'{path_to_save}.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.figure().clear()\n",
    "    plt.close()\n",
    "    plt.cla()\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN_HOP_LENGTH = 512\n",
    "N_FFT = 2048\n",
    "\n",
    "emotions = {\n",
    "    'fear': 0,\n",
    "    'happy': 0,\n",
    "    'sad': 0,\n",
    "    'surprised': 0,\n",
    "    'angry': 0,\n",
    "    'disgusted': 0,\n",
    "    'neutral': 0,\n",
    "    'calm': 0\n",
    "}\n",
    "!mkdir -p train_mel_pow/{fear,happy,sad,surprised,angry,disgusted,neutral,calm}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_WITH_AUG = False\n",
    "\n",
    "for _, image in data_paths.iterrows():\n",
    "    print(f\"{image.Emotion}: {image.Path}\")\n",
    "    some_sound, sampling_rate = librosa.load(image.Path)\n",
    "\n",
    "    # Clear data\n",
    "    create_mel(\n",
    "        some_sound, f'train/{image.Emotion}/{emotions[image.Emotion]}', 'mel')\n",
    "\n",
    "    # Augmented data\n",
    "    if GEN_WITH_AUG:\n",
    "        create_mel(noise(some_sound),\n",
    "                   f'train/{image.Emotion}/{emotions[image.Emotion]}_n', 'mel')\n",
    "        create_mel(stretch(some_sound),\n",
    "                   f'train/{image.Emotion}/{emotions[image.Emotion]}_s', 'mel')\n",
    "        create_mel(pitch(some_sound, sampling_rate),\n",
    "                   f'train/{image.Emotion}/{emotions[image.Emotion]}_p', 'mel')\n",
    "        create_mel(shift(some_sound),\n",
    "                   f'train/{image.Emotion}/{emotions[image.Emotion]}_sh', 'mel')\n",
    "\n",
    "    emotions[image.Emotion] += 1\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d757c388c1d2f8ed104bae42d6e1feadcbab4afb2a6f28c443141b8249f513d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('bachelor': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
